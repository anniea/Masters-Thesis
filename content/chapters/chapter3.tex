%===================================== CHAP 3 =================================

\chapter{Related Work}

This chapter presents a selection of the work related to our research, in terms of existing visualization tools and face recognition systems.

\section{Visualization Tool}

This section will present existing visualization tools for \acrshortpl{ann}. The advantageous features of each tool will be described, as well as their limitations, before ending the section with a discussion of the improvements offered by our approach. 

\subsection{Existing Visualization Tools}

We describe three prominent visualization applications for \acrshortpl{ann}, namely TensorBoard\footnote{https://www.tensorflow.org/get\_started/summaries\_and\_tensorboard}, \acrfull{digits}\footnote{https://github.com/NVIDIA/DIGITS}, and the Deep Visualization Toolbox\footnote{http://yosinski.com/deepvis}. While all are functional, the former two are being developed further, both improving existing features and adding new ones. Both of these are backed by large companies, with Google supporting the open-source project TensorFlow, and by extension TensorBoard, and NVIDIA developing \acrshort{digits}\footnote{https://developer.nvidia.com/digits}. The Deep Visualization Toolbox was developed as part of a research paper and is not receiving any support or updates \cite{yosinski-deepvis}. Other visualizations tools found had either too narrow a scope or too rudimentary an implementation to be considered. While the three tools differ greatly, their main purpose coincide: to aid in the creation and understanding of an \acrshort{ann}.

\subsubsection{TensorBoard}

TensorBoard is a suite of visualization tools for networks built with the TensorFlow library. The purpose of TensorBoard is to make it easier to understand, debug, and optimize \acrshortpl{ann}. It can be used to visualize TensorFlow graphs, which is how TensorFlow networks represent computations, providing a user with an architectural and computational overview. TensorBoard can also be used to plot network metrics over time, such as how the learning rate decreases or the loss changes during training, and other statistics, like the distribution of weight values. The user activates this by annotating the nodes of the graph with so-called summary operations, which prompts TensorBoard to write network summary data to a log and then produce real-time visualizations of its content. TensorBoard can also visualize high-dimensional data, like embeddings, through an interactive user interface.

\subsubsection{\acrshort{digits}}

The NVIDIA Deep Learning GPU Training System, also known as \acrshort{digits}, is a web application developed for managing deep \acrshortpl{ann} with input in the image domain. Originally, it only supported networks created with the Caffe\footnote{http://caffe.berkeleyvision.org/} library. A drawback with Caffe is that the network architecture needs to be defined in plain text configuration files, which can be tedious, especially for larger networks. \acrshort{digits} simplifies this process by providing users with an intuitive interface for executing many of the cumbersome tasks that usually requires manipulating the configuration files directly. At a later point, the application expanded its support to include the scientific computing framework Torch\footnote{http://torch.ch/}. The application allows users to upload and manage both datasets and network models, and also makes pre-trained models, such as AlexNet \cite{alexnet} and GoogLeNet \cite{googlenet}, available for use. For their uploaded networks, users can schedule, monitor, and manage training jobs, and view real-time visualizations of the accuracy and loss. In addition, \acrshort{digits} is able to execute networks on uploaded input, providing an output result, and simple visualizations of the weights and activations of each layer. \\

\noindent As mentioned, \acrshort{digits} is still being developed and its has been upgraded since the start of our Specialization Project. The developers have previously stated that they had no plans towards adding support for more frameworks, including TensorFlow\footnote{https://github.com/NVIDIA/DIGITS/issues/967}. However, in Q2 2017, the \acrshort{digits} website\footnote{https://github.com/NVIDIA/DIGITS} was updated with a message stating that TensorFlow support would be available in July 2017.

\subsubsection{Deep Visualization Toolbox}

The Deep Visualization Toolbox is a software tool that provides an interactive visualization of a trained \acrshort{ann} as it responds to user-provided input. The input can either be an uploaded image or video from a live web camera feed. Users can choose to use the included default network or opt for a network of their own making. The latter requires additional computation to produce the more complex visualizations in advance. The toolbox only supports networks created with Caffe. The toolbox allows users to cycle through layers and view the visualizations made for each one. These are either the activation output of the layer or a synthesized image that causes high activations in the layer units. In addition, a specific filter can be selected to be explored further. For a chosen filter, the toolbox can show nine synthesized images that induces activation, the nine images from the training set that activates the filter the most, and the feature patterns responsible for the high activations in each top image. The toolbox uses the visualization technique described in \autoref{sec:deep-vis} to create the synthesized images, and the technique from \autoref{sec:deconv-net} to produce the nine activation inducing feature patterns. Of all the visualizations, only the layer activations are computed in real time. The others are precomputed, due to the large cost involved in their creation or collection.

\subsection{Visualization Tools as Influential Factors}

When investigating the different visualization tools, we were most interested in which frameworks were supported and what functionality they offered. \acrshort{digits} have the widest support and can be used with networks built using Caffe, Torch and, come July, TensorFlow. In contrast, TensorBoard and the Deep Visualization Toolbox are restricted to a single framework each. To utilize TensorBoard, networks must use TensorFlow. For the Deep Visualization Toolbox, Caffe is the required framework. In terms of functionality, each of the three tools provide their own characteristic functionality, with a slight overlap of the most elementary visualizations. \\

\noindent A substantial difference of the Deep Visualization Toolbox compared to the other two tools, is that it can only be employed on fully trained networks. Rather than assisting the user in training, it focuses on deepening the understanding of the internal mechanisms of a network. TensorBoard and \acrshort{digits}, however, are both intended to be used during training, to help evaluate a network as it progresses. \acrshort{digits} is also capable of facilitating the whole creation and training process, and provides an organized system for managing datasets and models. Furthermore, \acrshort{digits} supports the use of fully trained networks. \\

\noindent Another distinction between the tools is the varying complexity of their visualization techniques. \acrshort{digits} provides the basic network visualizations, namely accuracy and loss plots throughout a training session. Additionally, it is able to visualize the layer weights and activations for a given input during network prediction. TensorBoard can also visualize the change in accuracy and loss, but focuses more on network metadata and statistics than the concrete values in the network. However, the most insightful visualizations are offered by the Deep Visualization Toolbox, which utilizes significantly more advanced techniques, such as deconvolutional networks and deep visualization. \\

\noindent When creating our own visualization tool, we build on the ideas of \acrshort{digits}, TensorBoard and the Deep Visualization Toolbox, aiming to bridge the gap between them. As we plan to visualize throughout the training process, we intend to adopt the basic visualizations of accuracy and loss found in TensorBoard and \acrshort{digits}. We also believe that the network management offered by \acrshort{digits} is highly valuable, and aspire to incorporate a similar feature in our tool. To be able to provide users with a deeper understanding of their networks, we intend to include the advanced visualization techniques from the Deep Visualization Toolbox as well. By employing the increasingly popular Keras \acrshort{api}\footnote{https://keras.io/}, we will be able to handle networks made with the Theano framework, which is not supported by any of the mentioned tools, and the TensorFlow framework, which is only supported by TensorBoard as of May 2017. In short, our tool will attempt to consolidate important functionality, and improve and extend framework support.


\section{Case Study in Face Recognition}

The proposed approach of exploiting facial expression in a face recognition network is a novel one. As a consequence, the earlier work related to the case study is scarce. Although we do not aspire to explicitly create a network that pushes the state of the art, we are interested in investigating a technique that might prove useful in future attempts to do so. This section therefore offers a brief overview of a selection of state of the art systems for facial recognition and their performance. Furthermore, we introduce an object classification network that has been repurposed for face recognition, and review its relevance to our case study. \\

\noindent The networks mentioned in this section are often described to be deep or even very deep. It is worth mentioning that the notion of depth in \acrshortpl{ann} has changed somewhat as the research on such networks has progressed. With the existence of ResNet-152 \cite{resnet}, an accomplished 152-layer deep \acrshort{ann}, networks with less than 20 layers seem shallow in comparison. However, these smaller network are still commonly referred to as deep. For consistency, the practice is continued in this thesis. 

\subsection{Improving Face Recognition} \label{sec:improving-face-recognition}

The area of face recognition is widely researched, and the application of \acrshortpl{ann} has enjoyed great success in recent years. A popular benchmark dataset is \acrfull{lfw} \cite{lfw}, and in 2015 a network called FaceNet achieved 99.63\% verification accuracy on the set \cite{facenet}. While this performance is remarkable, even slight increases in accuracy are highly valued, as is evident when considering the previous state of the art, the DeepID3 network, which achieved 99.53\% accuracy on \acrshort{lfw} \cite{deepid3}. It is clear that the research area is in a state of diminishing returns, where considerable amounts of work is required to produce minor improvements. Further evidence of this can be found in earlier work, with the DeepFace network increasing state of the art accuracy from 96.33\% to 97.35\% \cite{deepface}. Consequently, if the incorporation of facial expression data in a network produces even a slight rise in accuracy, the results are interesting. It is worth noting that not all of the promising results belong to \acrshortpl{ann}, with the GaussianFace model obtaining 98.52\% accuracy on \acrshort{lfw}, being the first system to surpass the verification accuracy measured for humans on the dataset (97.53\%) \cite{gaussianface}.

\subsection{Using \acrshort{ilsvrc} Networks for Face Recognition}

While all the aforementioned state of the art \acrshortpl{ann} have their own distinct network architectures, common characteristics are that the networks are both convolutional and deep. This is also a distinction shared by several top performing networks entered in the \acrfull{ilsvrc} \cite{imagenet} over the years \cite{inception, vgg, googlenet}. These \acrshort{ilsvrc} networks are interesting because of their powerful feature extracting capabilities and performance on varied object recognition in unconstrained environments. These qualities make them excellent choices for transfer learning networks. The use of deep \acrshortpl{cnn} in \acrshort{ilsvrc} was initially inspired by the so-called AlexNet \cite{alexnet}, which won the object classification category with a 15.3\% test error rate against the 26.2\% error rate of the closest competitor. The victory triggered an interest in what such networks were capable of, leading to successful applications in multiple computer vision fields, including face recognition. \\

\noindent In Parki et al. \cite{deep-face-rec}, several \acrshortpl{ann} based on an \acrshort{ilsvrc} network are tested in the face recognition domain. More specifically, the networks are modelled after the \acrfull{vgg} architecture, which was awarded second place in the object classification task of \acrshort{ilsvrc} 2014 \cite{vgg}. To train the face recognition networks, Parki et al. \cite{deep-face-rec} constructed a dataset of celebrity images, consisting of 2622 identities with 1000 images each. The networks were tested on \acrshort{lfw} and YouTube Faces Dataset \cite{youtube-set}, where the best accuracies achieved were 98.95\% and 91.6\%, respectively. These scores are comparable to those achieved by the state of the art networks DeepFace, DeepID3 and FaceNet, mentioned in \autoref{sec:improving-face-recognition}.

\subsection{Application to Case Study} \label{sec:vggface-application}

For our case study network, we intend to employ the \acrshort{vgg} architecture as a feature extractor. It has proven capabilities in both complex object classification and face recognition tasks, as well as having a fairly straightforward architecture. The latter is beneficial as lower complexity makes reasoning about experimental choices less complicated and reduces the possibility of unforeseen effects. However, \acrshort{vgg} networks are prohibitively slow to train. To circumvent this issue, an option is to apply transfer learning to a fully trained version of a \acrshort{vgg} network. The networks from Parki et al. \cite{deep-face-rec} are prime candidates because of their impressive performance in a related problem area.

\cleardoublepage