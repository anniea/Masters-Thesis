%===================================== CHAP 6 =================================

\chapter{Discussion}

This chapter will discuss the results presented in the previous chapter in order to answer the research questions presented in chapter X.X.

\begin{comment}

RQ 1:How can we develop a visualization tool to improve the understanding of the behaviour of an artificial neural network?

RO 1.1: Examine existing tools in order to uncover shortcomings.
RO 1.2: Identify visualization techniques that can be used to improve the understanding of an artificial neural network.
RO 1.3: Develop a tool that incorporates the techniques found in 1.2 and addresses the shortcomings identified in 1.1.

\end{comment}

\section{Visualization Tool}

In this section we will interpret and explain each of the visualizations presented in the previous chapter, before ending with a justification of how our approach answers RQ1 defined in chapter X.X.

% bedre skille mellom denne introen og intro på første subsection.
% skriv hele denne introen på nytt egentlig

% examination?

% Interpret and explain your results
% Answer your research question
% Justify your approach
% Critically evaluate your study

\subsection{Visualization Techniques}

% for både mnist og vgg på hver teknikk

\subsubsection{Training Progress}

The only produced visualizations of the training progress were the batch accuracy and batch loss plots of the MNIST network in \textbf{Fig. \ref{mnist:accuracy}} and \textbf{Fig. \ref{mnist:loss}}, respectively. Both plots show a healthy network, reaching an accuracy of close to 100\% shortly after training for three epochs. Even though the training was continued for another three epochs, the network show no signs of overfitting as the validation accuracy keeps being high. 

%For a simple example like the MNIST toy problem, however, ...

\subsubsection{Layer Activations}

In \textbf{Fig. \ref{mnist:layer-act-all}}, we see the layer activations of most layers of the MNIST network. The first convolutional layer does not seem to detect...

\begin{comment}
FIG. 1.5:
- We see that the first convolutional layer do not find any specific features
- It is not until the second convolutional layer that we see features in terms of edges.
- Fc 3: the network is 100\% certain that it is class 0.

FIG. 1.6:
- Shows conv2 at 1, 5 and 10.

Each square in the montage of activations is the output of a channel in the conv1 layer. White pixels represent strong positive activations and black pixels represent strong negative activations. A channel that is mostly gray does not activate as strongly on the input image. The position of a pixel in the activation of a channel corresponds to the same position in the original image. A white pixel at some location in a channel indicates that the channel is strongly activated at that position.

This technique can be used to determine what kinds of features a convolutional network learns at each layer of the network. 

One dangerous pitfall that can be easily noticed with this visualization is that some activation maps may be all zero for many different inputs, which can indicate dead filters, and can be a symptom of high learning rates.
--> ikke mulig å se flere inputs samtidig

\end{comment}

\subsubsection{Saliency Maps}

The evolution of the saliency maps throughout training for the MNIST network is shown in \textbf{Fig. \ref{mnist:saliency}}. We see that at stage 0, which is before any training has been conducted, the saliency appears to be randomly computed, except for the black areas where the gradients are zero. The random computation of the saliency is coherent with the random prediction of the input image as the network has not seen any images yet.\\

\noindent Looking at the following stages, it is evident that the most prominent changes happen during the first stages of training. From stage 7 to stage 10, there are only small adjustments present. The reason for this is that the loss stabilizes after some time. Recall that the saliency is the gradient of the loss with respect to the input image, thus as the loss stabilizes so will the gradient and consequently the saliency. \\

\noindent As seen in \textbf{Fig. \ref{mnist:input-images}}, all visualization input images were classified correctly at stage 1, except for digit 5. Looking at the stage 1 saliency map of digit 5, we see two highlighted spots at the top right and bottom left. These are the parts that the network deems as most important when classifying the input image. In this case, the input image was classified as the digit '2', and we see that the highlighted areas are somewhat consistent with the shape of the number 2, as well as the highlighted areas of the stage 1 saliency map for digit 2. \\

% avslutt dette avsnittet - gir mening at nettverket klassifierte som 2
% bildet for 5 er litt uvanlig. skiller seg kanskje ut blant de 5-tall den er trent på - ikke generell nok enda.

\noindent Another interesting observation is that for many of the input images, the first couple of stages resembles the input image the most. For instance, for digit 3, the saliency map of stage 1 tracks the input image fairly well, while stage 10 presents a significantly more noisy and confusing saliency map. As the training progresses, the network sees more and more images and thus it generalizes to these images. The final saliency maps show us that the network could be looking mostly for certain characteristics instead of the whole picture when classifying the numbers. This is especially evident for digits 1, 4 and 7. For digit 1, we see that its last couple of saliency maps does not highlight the digit from the very bottom to the top. It focuses on the middle part of the line. This could be because the 1s that the network has seen all are of various length and thus the common and most important part influencing the classification is the middle. Similarly, the final saliency maps of digit 4 focus on the two vertical top lines, while the final saliency maps of digit 7 focus on the top horizontal line and practically ignores the diagonal line. \\

\noindent In addition to the saliency maps for the MNIST network, three saliency maps were also presented for the VGG16 network in \textbf{Fig. \ref{vgg:saliency}}. The input images of VGG16 are far more complex than those of MNIST, which is also evident in their saliency maps. It is hard to see any obvious shapes in the saliency maps, but they still indicate what areas of the image were most important. Note that the network was uncertain in its classification of the last two images, which can also explain why the saliency maps seem confusing. However, the top image was classified as a tricycle with 99.95\% certainty. Its saliency map shows us that the focus was definitely on the little girl and especially on how her legs and arms are positioned. This may tell us that the network % mener at et barn som sitter 

% noe om at nettverket tenker at et barn som sitter på denne måten betyr at det er en tricycle. kan også ta med noe om den streken, at det er ved en gatekant eller noe.

%many images of tricycles also includes children sitting on the tricycle. The slightly skewed vertical line through the saliency map can 

%a: tricycle 99.95
%b: cliff 64.35
%c: sombrero 50.91

\begin{comment}
The saliency map indicates to what degree each pixel in the visualization image influences the specific class outcome. The important regions are easily identified by their brightness. The saliency map can help you to see which parts of the visualization image that your network deems more important when deciding upon the classification score chosen.
\end{comment}

\subsubsection{Deconvolutional Network}

For the MNIST network, only four simple visualizations produced using a deconvolutional network were presented. The visualizations were shown in \textbf{Fig. \ref{mnist:deconv}}, and clearly illustrate that the feature maps are looking for simple edges at various locations in the input image. \\

\noindent \textbf{Fig. \ref{vgg:deconv-block5}} presents visualizations using a deconvolutional network for feature maps in the last pooling layer of the VGG16 network. Naturally, these feature maps seem to be looking for more complex features such as wheels (number 348, 422 and 450) and faces (number 380). In \textbf{Fig. \ref{vgg:deconv-block5-comparison}} we can confirm our interpretation for feature map number 348 and 380. The feature maps surely focus on the wheels and face present in the two images. We can be even more specific in our interpretation, and say that feature map number 348 looks for front wheels of bicycles, which is consistent with both visualizations of the specific feature map. \\

\noindent In \textbf{Fig. \ref{vgg:deconv-block3-comparison}}, we see visualizations using the deconvolution technique for a lower layer. The visualizations shows that the feature maps of this layer looks for more specific, but still complex, features. The selection presents feature maps that are looking for wheels. The comparison shows that the feature maps looks for wheels in similar locations and for similar regions. \\

\noindent Visualizations for the lowest pooling layer of the VGG16 network are seen in \textbf{Fig. \ref{vgg:deconv-block1}}. These feature maps are looking for basic features such as colors and edges. For instance, feature map number 18 obviously looks for the color pink, feature map number 29 looks for a thin line, and number 47 for a rounded corner, and so on.

\begin{comment}
The visualizations show the pattern in the visualization image that was responsible for eliciting activations in the specific feature map. This pattern can reveal what a certain part of the network finds important and what it is looking for in an image. The tools of the figures are linked together similar to the saliency map figures. Additionally, if you click the save tool, all of the deconvolutional visualizations will be downloaded.
\end{comment}

\subsubsection{Deep Visualization}

% utvikler seg mer i starten enn mot slutten
% variasjoner i siste steg - viser hva som går igjen.
% ser litt av det samme som i saliency maps - f.eks. for 4 er det toppen som er viktigst. blir også mer generaliserte etterhvert.


\begin{comment}
The page presents a grid of figures, with a synthesized image for each network unit chosen for visualization. The synthesized images are optimized to maximally activate their corresponding unit. In other words, they show what the selected units are looking for in an image.
\end{comment}

\subsection{Visualizations in the Visualization Tool}

% det er åpenbart at visualiseringene kan gi oss innsikt i hvordan et nettverk fungerer.
% ser også at visualisering underveis i treningsprosessen gir innsikt i utviklingen
% ofte er det nyttig å sammenlikne bilder, som vi ser i resultatene - ikke mulig. (future work)
% kan også være nyttig å kunne fjerne og legge til visualisering for units/layers underveis (future work)
% vanskelig (?) å konstruere et eksempelnettverk som ikke gjør det bra slik at man kan se dette i visualiseringene - vanskelig å peke på spesifikke ting som er dårlig.
% fokuset har vært på å få en forståelse - denne forståelsen kan helt sikkert brukes til å forbedre et nettverk som ikke performer like bra som den kan.

\subsection{Discussion} % kalle denne noe annet

% lettere å se når du sammenlikner?

\section{Case Study in Face Recognition}


% for extra output: loss is high, but accuracy isnt THAT bad

% The combined metrics of the extra output architecture make it harder to compare to the baseline and extra input architecture.

% The extra output architecture underperforms in comparison with the other two, but still manages an impressive accuracy considering it is tackling two problems simultaneously.

\cleardoublepage