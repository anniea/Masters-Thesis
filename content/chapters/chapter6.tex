%===================================== CHAP 6 =================================

\chapter{Discussion} \label{chap:discussion}

This chapter will discuss and evaluate the results presented in \autoref{chap:results}, and reflect upon the findings.

\section{Visualization Tool}

In this section we start with interpreting and explaining each of the visualizations presented in \autoref{sec:mnist-vis-results} and \autoref{sec:vgg-vis-results}. The visualizations of both the \acrshort{mnist} network and the \acrshort{vgg} network will be examined for each visualization technique. The section ends with a discussion on the incorporation of the visualizations into the implemented visualization tool.

% Interpret and explain your results
% Answer your research question
% Justify your approach
% Critically evaluate your study

\subsection{Training Progress}

The only produced visualizations of the training progress were the batch accuracy and batch loss plots of the \acrshort{mnist} network in \autoref{fig:mnist-training-progress}. Both plots indicate a healthy network, reaching a training accuracy of close to 100\% shortly after three epochs of training. Even though the training continued for another three epochs, the network shows no signs of overfitting as the validation accuracy is still high.

\subsection{Layer Activations}

In \autoref{fig:mnist-layer-act-all}, we see the layer activations for most layers of the \acrshort{mnist} network. The first convolutional layer does not seem to detect particularly interesting features, but the second convolutional layer apparently detects edges in various locations, which is typical for the first couple of layers of a \acrshort{cnn}. The black feature maps indicate that their corresponding filters respond to features that are not found in the selected visualization input image. If the feature map remains black for several input images, it may indicate a dead filter, possibly caused by too high a learning rate. The rest of the layer activations present an overview of the network behavior resulting in a correct classification of the image with 100\% certainty, seen as a single bright activation in the output layer. \\

\noindent \autoref{fig:mnist-layer-act-conv2} shows how the layer activations develop as the training progresses. At stage 1, the convolutional layer has not yet learned to identify features. We see a significant improvement in stage 5, where the edges of the visualization input image emerge. From stage 5 to stage 10, there are no major improvements, as the network has stabilized. \\

\noindent In \autoref{fig:vgg-layer-act}, we see the activations in the second convolutional layer of the \acrshort{vgg} network. By studying the feature maps and comparing them to the visualization input image, we can conclude that they do not only detect edges, but also other features, like color. For instance, the third feature map on the third row seems to activate greatly on the color pink, and some of the other feature maps detect the green color of the grass.

\subsection{Saliency Maps}

The evolution of the saliency maps while training the \acrshort{mnist} network is shown in \autoref{fig:mnist-saliency}. We see that at stage 0, which is before any training has been conducted, the saliency appears to be randomly computed, except for the black areas, where the gradients are zero. The random computation of the saliency is coherent with the random prediction of the input image as the network has not seen any images yet. Looking at the following stages, it is evident that the most prominent changes happen during the first stages of training. From stage 7 to stage 10, there are only small adjustments present. The reason for this is that the loss stabilizes after some time. Recall that the saliency is the gradient of the loss with respect to the visualization input image. Thus, as the loss stabilizes, so will the gradient and, consequently, the saliency. \\

\noindent As seen in \autoref{fig:mnist-input-images}, all visualization input images were correctly classified at stage 1, except for digit 5. Looking at the stage 1 saliency map of digit 5, we see two highlighted spots at the top right and lower middle. These are the parts that the network deems most important when classifying the visualization input image. In this case, the image was incorrectly classified as '2'. We can see that the location of the highlighted areas are somewhat similar to those of the stage 1 saliency map for digit 2. The network has learned a representation for '2' that it finds similar to digit 5, and it classifies accordingly. Note that the visualization input image for 5 is a particularly poorly written example. These kinds of images are necessary to fully generalize the network, but at an early stage of training, the network has not seen enough images to tackle them. At stage 2, the network has already improved and is able to correctly classify the digit 5. \\

\noindent Another interesting observation is that for many of the visualization input images, the first couple of stages resembles the image the most. For instance, for digit 3, the saliency map of stage 1 tracks the image fairly well, while stage 10 presents a significantly more noisy and confusing saliency map. As the training progresses, the network sees more and more images and thus it generalizes. The final saliency maps show us that the network could be looking mostly for certain characteristics instead of the whole picture when classifying the digits. This is especially evident for digits 1, 4 and 7. For digit 1, we see that its last couple of saliency maps does not highlight the digit from the very bottom to the top, but focuses on the middle part of the line. This could be because the 1s that the network has seen vary in length, and thus the middle is the common and most important part influencing the classification. Similarly, the final saliency maps of digit 4 focus on the two vertical top lines, while the final saliency maps of digit 7 focus on the top horizontal line and practically ignores the diagonal line. These are characteristics that distinguish them from the other digits, and consequently helps the network in its classification. \\

\noindent In addition to the saliency maps for the \acrshort{mnist} network, three saliency maps are also presented for the \acrshort{vgg} network, in \autoref{fig:vgg-saliency}. The visualization input images of \acrshort{vgg} are far more complex than those of \acrshort{mnist}, which is evident in their produced saliency maps. It is hard to see obvious shapes in the saliency maps, but they still indicate what areas of the images were the most important. The top image was classified as a tricycle with 99.95\% certainty. Its saliency map shows us that the focus was on the little girl and especially how her legs are positioned. This tells us that the network considers the specific pose as an indication of a tricycle. Most likely, many of the images it has seen of a tricycle have included a child sitting on it. The saliency map also slightly highlights the curb, which may be another hint towards a tricycle. The middle image was classified as a cliff with 64.35\% certainty. Since the image also contains a bicycle and a man, it is difficult for the network to know which object to focus on. In the corresponding saliency map, we see that the man is highlighted along with parts of the rock and background. This can be expected, as the network has most likely been exposed to many cliff-labeled images in training which contained people posing for a photo, as well as a scenic backdrop. Naturally, the texture of the rock is also important for the classification. Since the saliency map is produced based on the chosen classification, the bicycle is not highlighted at all. A possible reason for the lack of a bicycle classification might be its unusual position, both in relation to the man and the background. The bottom image is incorrectly classified as a sombrero. However, the network is only 50.91\% certain of this classification. When looking at the visualization input image, the shape of the watermelon plate does resemble a sombrero, and the saliency map highlights an area that is consistent with such a shape. We also see that the head of the woman is highlighted, and signifying that the network has learned that faces are prominently featured in images of sombreros.

\subsection{Deconvolutional Network}

For the \acrshort{mnist} network, four simple visualizations produced using a deconvolutional network were presented. The visualizations are shown in \autoref{fig:mnist-deconv}, and clearly illustrate that the feature maps are looking for edges at various locations in the input image. As described in \autoref{sec:example-networks}, the \acrshort{mnist} network is not particularly suitable to showcase the deconvolution technique, since the network is fairly small. The \acrshort{vgg} network is significantly larger, and is therefore a better option for demonstrating the visualization technique. \\

\noindent \autoref{fig:vgg-deconv-block5} presents visualizations from a deconvolutional network for feature maps in the last pooling layer of the \acrshort{vgg} network. Naturally, these feature maps seem to be looking for complex features such as wheels (number 348 and 450) and faces (number 380). In \autoref{fig:vgg-deconv-block5-comparison} we can confirm our assumptions for feature map number 348 and 380, as they focus on the wheels and face present in the other two visualization input images as well. \\

\noindent In \autoref{fig:vgg-deconv-block3-comparison}, we see visualizations using the deconvolutional technique for a lower layer. The visualizations shows that the feature maps of this layer search for more specific, but still complex, features. The comparison shows that the feature maps are looking for different parts of a wheel. Visualizations for the lowest pooling layer of the \acrshort{vgg} network are seen in \autoref{fig:vgg-deconv-block1}. These feature maps are searching for basic features such as colors and edges. For instance, feature map number 18 obviously looks for the color pink, feature map number 29 looks for a thin line, and number 47 for a rounded corner, and so on. 

\subsection{Deep Visualization}

\autoref{fig:mnist-deepvis-output} shows the evolution of the deep visualizations for the various visualization input images of the \acrshort{mnist} network. The same trend is evident here as in the other techniques described: the biggest changes happen between the first stages. We also see that as the training progresses and the network sees more examples, the deep visualizations actually seem to look less like the actual number. For instance, digits 1 and 5 start looking slightly distorted at stage 10. This may be because the network is getting more generalized and learns that the digits can have large variations. The digit 1 might be written straight from top to bottom, or slightly tilted from right to left. Another detail worth noting is that the right line of digit 4 is much more noticeable on the upper half than on the bottom half. We saw this trend in the saliency maps as well, where the network deemed some regions of the digits more important than others. It is evident that the bottom of the line is not a critical factor for classifying a digit as 4, likely because it is present in other digits as well, such as 7 and 9.\\

\noindent Deep visualizations of units in a lower layer of the \acrshort{mnist} network are shown in \autoref{fig:mnist-deepvis-fc2}. The visualization of the units of the lower layers indicate how the characteristics found in the output unit visualizations are constructed. We can see patterns in these visualizations that resemble parts of the visualizations in the output layer, but none of them look like complete digits. For instance, unit 24 resembles the stage 10 visualizations of digit 7. Unit 0 looks like digit 0 with its rounded parts in the bottom left and top right, but it also shows a vertical line at the top similar to the vertical line of digit 7. Unit 40 shows four white dots at the top that many of the output visualizations seem to contain at least one of, e.g. digits 2, 3, 4 and 6. The black unit 56 is most likely the result of a dead filter. \autoref{fig:mnist-deepvis-fc1} displays deep visualizations of units in an even lower layer. Here, unit 0 resembles the previously described unit 40, and unit 96 looks somewhat like the digit 3. \\

\noindent The deep visualizations of selected units in the output layer of the \acrshort{vgg} network, seen in \autoref{fig:vgg-deepvis-output}, are significantly more complicated. Most of the output classes are somewhat straightforward to identify. For instance, the tarantula class visualization contains furry, insect-like legs and prominent spider shapes, and the starfish class visualization shows star-shaped objects with a scale-like texture. In the bow tie class visualization, there are shapes clearly resembling bow ties. In addition, various patterns such as stripes, dots and plaid can be identified. This illustrates the invariance of the class, in that a bow can have many different patterns. Some of the visualizations, however, contain parts that we may not immediately see as directly related, but that are common in the context of the class. A good example is the volleyball class visualization. There are several round shapes in the image, but they are not particularly dominant. We can also see what resembles hands, stretching up from the bottom. In addition, the visualization is covered in a texture that appears to be a net. This shows that it is the combination of several objects that results in a classification. The presence of other objects related to a specific output class improves the certainty that the image belongs to that class. This can also be seen in both the bubble and umbrella class visualizations. Parts of faces are noticeable, because there is usually a person blowing the bubbles, or a person standing beneath the umbrella. The umbrella class visualization also contains the texture of raindrops, which makes sense for umbrellas. \\

\noindent The deep visualization technique incorporates an element of randomness into the creation of visualizations. This is shown in \autoref{fig:vgg-deepvis-output-variations}. The tarantula class visualization is produced twice with the exact same hyperparameter settings. We see that the images are not identical, but they definitely illustrate similar characteristics. Each of the images represent what the chosen unit is looking for, and the variance in the produced images represent the network's invariance. In \autoref{fig:vgg-deepvis-output-hyperparameters}, four versions of the tarantula class version is produced, this time with various combinations of hyperparameters. These show how the hyperparameters can be altered to emphasize different characteristics, like the high frequency information in the top-right image or the most crucial patterns in the bottom-right image. The top-left images display a small set of important regions, while the bottom-left reveals typical low frequency patterns. \\

\noindent \autoref{fig:vgg-deepvis-fc2} shows visualizations of the second fully connected layer. These are more challenging to interpret, since we do not have a definite answer of what the unit is looking for, like we did in the output layer. Also, the extent of the output classes makes it difficult to identify similarities to the output class visualizations. By studying the visualizations closely, we can still speculate on what the units are looking for. The first two visualizations seem to resemble birds of some kind, which is reasonable knowing that there are a number of different bird species among the output classes. The third visualization contains something that resembles leaves or insect wings. \\

\noindent In \autoref{fig:vgg-deepvis-fc1}, we can see the visualizations of the first fully connected layer. These appear to be more intense and less specialized than the second fully connected layer, which can be attributed to the fact that it is directly connected to every unit in the last convolutional layer. Consequently, it has the potential to detect a wide variety of features at once. In unit 2048, we see the resemblance of furry bodies, while unit 3072 show parts of snake heads, particularly at the top left corner. Recall that the features of the units in underlying layers are further combined in order to construct the final features of the output layer. The lower layer units would then contain features that may appear in several different output classes. \\

\noindent \autoref{fig:vgg-deepvis-blocks} illustrate how the deep visualizations change in size and complexity as they go further down into the network. Units in block 2 clearly identifies simpler features than block 5. In \autoref{fig:vgg-deepvis-block5-lr-variations}, we see how changing the learning rate and number of iterations affect the units of a lower layer. Typically, a higher learning rate and more iterations lead to more pronounced features in the visualizations, but at a certain point they are oversaturated, as seen in the rightmost image in \ref{fig:vgg-deepvis-block5-lr-variations}. Finally, \autoref{fig:vgg-deepvis-block1} once again confirms that the lower layers look for the more fundamental features. For instance, unit (56, 56, 8) looks for pixels with a prominent value compared to its surroundings, and unit (56, 56, 48) detects edges.

\subsection{Usefulness of the Visualization Tool}

The visualization tool incorporates the presented visualization techniques in a simple web interface, as seen in the user manual in \autoref{app:user-manual}. It allows users to browse the various visualizations while the network is training. By interpreting the results of \autoref{sec:visualization-tool-results}, we have seen that there is undoubtedly insight to be gained from using these visualization techniques. For a fully trained network, the visualizations provide indications on how the \acrshort{ann} is behaving and why it behaves as it does. We have also seen that examining the visualizations during training can deepen the understanding of how the network progresses and when the learning process seem to be at an end, marked by diminishing changes to the visualizations. The training progress provides a straightforward indication on how well the network is performing, while the other techniques focus on uncovering what the various units in the network are actually looking for or find important, using different perspectives. The knowledge acquired from studying each of the visualization techniques may overlap, but by combining the various approaches, we can gain a greater overall intuition. \\

\noindent We chose to showcase the visualization techniques using well-known example \acrshortpl{ann}, which are already known to achieve good results, to concentrate our focus on what the visualizations reveal about their behaviour. The visualizations provide insights into the networks, whether they perform well or poorly. Understanding how a network behaves, and why, makes it possible to identify shortcomings of less successful networks, and further to address these by making informed decisions on improvements. \\

\noindent Many of the resulting visualizations were presented side-by-side, as comparison can be useful and lead to even further insight. Although looking at visualizations for a number of visualization input images is helpful, the tool currently only allows a single visualization input image for each network. This feature was given a lower priority compared to supplying a wide range of visualization techniques and making them easy to use.\\

\section{Case Study in Face Recognition}

The case study networks included in \autoref{sec:case-results} do not all represent the most promising architectures. We have three architecture types: the baseline, the architecture with extra input, and the architecture with extra output. For each of these, three configurations were included: a minimal, a preferred, and an interesting alternative. The minimal configurations were A, D and G, while the preferred were B, E and H, and lastly, the alternate ones were C, F, and I. Before comparing the three architecture approaches, we first compare the chosen configurations within each approach.

\subsection{Baseline Networks}

The baseline networks were created using configurations A, B and C. The difference between these networks is their depth, where B is deeper than A, and C is deeper than B. As can be seen in \autoref{tab:case-results}, the accuracy achieved by the minimal network A is relatively high. This, combined with the fact that this network only has a single weight layer, demonstrates the network's ability to easily discern valuable information from the features outputted from the pretrained network. By extension, it confirms the pretrained network's suitability as a feature extractor. To improve the performance, B adds depth, resulting in a more complex classifier. The increased complexity allows the network to interpret the feature input more thoroughly and better tune itself towards images in the \acrshort{imfdb} dataset. The result is a rise in accuracy and a decline in loss compared to A. C, however, which is the deepest baseline network, achieve poorer results than B in general, and has larger loss values than A. A reasonable explanation is that the added parameters enable the network to overfit to the small dataset. The improved accuracy over A is likely caused by outlier examples being correctly classified with low confidence.

\subsection{Extra Input Networks}

Configurations D, E and F were used to create the extra input networks. These differ not only in depth, but also in where the additional input is supplied. E and F have the equally deep, with D being shallower than both. Using a minimal configuration, D has no other option than to inject the extra input right at the start of the network. E and F introduce the expression input at different depths, which marks the main distinction between their architectures. From \autoref{tab:case-results}, we can see that the complexity gained by adding more layers has a beneficial effect on performance, as both the deeper networks outperform the shallow one, in all aspects. The network from configuration D has inferior performance due to its sole weight layer, the output, which needs to make sense of the combined inputs without any additional processing. E and F does have extra processing, but distinct kinds of processing. In F, the expression input is provided early on in the network, forcing every weighted layer in the model to process the information from the feature and expression inputs simultaneously. E, however, injects the expression input later, leaving earlier layers free to exclusively process the feature input. With the performance of E being unequivocally better than F's, the processing of feature input in isolation appear to have advantageous effects. A possibility is that the computations on the feature input alone refines the features, which then eases the subsequent simultaneous processing. 

\subsection{Extra Output Networks}

The extra output network were based on architecture configurations G, H and I. They have varying depth, and different specialized processing parts. H and I are equally deep, both having more layers than G. These two are also similar in the early parts of the networks, with two weighted layers that are used for classifying both identification and expression. However, H utilizes additional specialized processing for the expression output, while I uses the equivalent for the identification output. G has no such components, and rather computes both classification outputs immediately. The identification and expression specific results can be found in \autoref{tab:case-results-id} and \autoref{tab:case-results-exp}, respectively. Together, these results reveal how the architectural decisions affect performance. \\

\noindent H stands out as the top performer, with superior values for both loss and accuracy when classifying identification. Looking at the expressions metrics, it also has the lowest expression output loss. Despite this, its accuracy in expression classification is not far off from I's. As mentioned before, this could be caused by the larger loss network, in this case I, correctly classifying outlier examples with low confidence. In H, the feature input is passed through two common weight layers before being presented to the separate output layers. These layers are influenced by both output layers during training, and are subsequently forced to produce information that is valuable to the two classification tasks simultaneously. It is here the use of expression data has the possibility to influence the performance of the network, by potentially revealing a pattern in how the variations in facial features that map to the same identity coincide with the variations in the expected expression outcome. The specialized processing for expression classification in H comes from an additional weight layer which is only affected by the expression output layer when training. Network I has similar shared layers as H, but uses specialized processing for the identification classification instead of for expressions. The common layers in I do improve expression performance compared to G, but, peculiarly, these layers combined with the specialized processing do not contribute to an definite advantage in identification classification. Again, I shows somewhat similar accuracy, but has a larger loss. As mentioned in \autoref{sec:case-networks}, the lack of an identification performance boost is likely caused by overfitting due to the specialized processing. 

\subsection{Comparing Approaches}

To compare the extra output approach to the others, we focus on the identification performance. When looking at the architectures of the networks, D and G resemble the baseline configuration A. Of these, D is the most successful performance-wise, with A and G showing similar performances. The latter comes as no surprise, considering that the input in G is not processed by any weight layers that are shared among the identification and expression output layers. Consequently, there are no common layers which both affect in the learning procedure, and the classification of identification and expression is executed strictly separately. Classifying the identification is then performed similarly in A and G. The sole difference between A and D is the extra input D receives. D's utilization of this additional information is the only architectural explanation for its increase in performance. \\

\noindent The situation is similar to how E and F compare to B. The enhanced performance of E and F over D solidifies the interpretation of B's improvement over A, that the added layers allow for increased complexity and potential tuning to our particular input. However, in the extra input architectures, the more complex systems have access to more information, and, as implied by the advance in performance, they find a meaningful use for it. With the addition of the expression information being the only architectural difference from B, it must be the cause of the performance gain. All of the baseline configurations can then be improved upon by including an additional expression input. \\

\noindent The last baseline configuration, C, is comparable to H and I. Among these, H has the most promising results, with C performing slightly above I, only notably surpassing it when regarding the loss calculated from the validation set. While H is as deep as C, its full depth is only utilized to classify the expression, and the identification result is computed after the first two weight layers. Since we are considering the extra output architectures based their identification accuracy, H could be compared to B, by disregarding the expression specific processing part. These two have quite similar results, with H having slightly higher accuracy, but also slightly larger loss values. Because of the meager differences, we cannot decisively say that one configuration is preferred over the other based on their performance. However, if we include complexity, B is the better choice. Not only does its architecture contain less parameters, it also requires less data in training. \\

\noindent Of the three approaches, the extra input architectures have the most impressive results. Every configuration in this approach outperforms their corresponding baseline alternatives. Compared to the extra output architectures, they are also generally superior, with the exception of the minimal configuration D versus H. However, the most interesting architectures consist of the preferred configuration of each approach, namely B, E and H. E is definitively the best choice among these, with overall better performance measurements, proving that the extra input architecture is the approach to prefer. In addition, B is favored over H, as previously discussed, making the extra output architecture the least promising alternative, even after the vanilla baseline approach. These results demonstrate that a face recognition system can be improved by exploiting facial expression data, and that an advantageous approach is to include the expression information as an additional network input.

\subsection{Experienced Limitations}

It is important to note that the performance gain of the extra input architectures was moderate, as was the performance differences in general. An explanation for this is the humble size of the various architectures and their similarity to each other. While these qualities make the current performance differences more significant, it also raises the question about how the experimental approaches would alter the behaviour of larger networks, if they would perform similarly or if the differences would grow or diminish. In our case study, the size of the dataset prevents us from examining this interesting question, as attempts at creating a larger architecture consistently resulted in overfitted networks, despite heavy regularization. Additionally, without a substantially larger dataset, we did not have the ability to train a sophisticated feature extractor from scratch. Rather, we were confined to using a pretrained model. The model chosen did not have any training on expression data, because of a distinct lack of such alternatives. \\

\noindent With a more extensive dataset, a larger network could be trained with which the experimental approaches could be explored further. We would for instance have more options on where to input the expression data for the extra input architectures, which has shown to have an impact on performance in the smaller networks. Similarly, we could expand the amount of specialized processing for each output when using architectures with an additional expression output. The extra output networks could also benefit from using a feature extractor that had expression data included in its training.


\cleardoublepage