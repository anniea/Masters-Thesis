%===================================== CHAP 6 =================================

\chapter{Discussion} \label{discussion-chapter}

This chapter will discuss and examine the results presented in chapter \ref{results-chapter}, in order to answer the research questions posed in section \ref{research-questions}.

% ta med research questions på nytt her?

\begin{comment}

RQ 1:How can we develop a visualization tool to improve the understanding of the behaviour of an artificial neural network?

RO 1.1: Examine existing tools in order to uncover shortcomings.
RO 1.2: Identify visualization techniques that can be used to improve the understanding of an artificial neural network.
RO 1.3: Develop a tool that incorporates the techniques found in 1.2 and addresses the shortcomings identified in 1.1.

\end{comment}

\section{Visualization Tool}

In this section we start with interpreting and explaining each of the visualizations presented in sections \ref{mnist-vis-results} and \ref{vgg-vis-results}. We end the section with a justification of how our approach answers RQ1:

In this section we will interpret and explain each of the visualizations presented in the previous chapter, before ending with a justification of how our approach answers RQ1 defined in chapter X.X.

% bedre skille mellom denne introen og intro på første subsection.
% skriv hele denne introen på nytt egentlig, mot slutten

% examination?

% Interpret and explain your results
% Answer your research question
% Justify your approach
% Critically evaluate your study

\subsection{Visualization Techniques}

% for både mnist og vgg på hver teknikk

\subsubsection{Training Progress}

The only produced visualizations of the training progress were the batch accuracy and batch loss plots of the MNIST network in \textbf{Fig. \ref{mnist:accuracy}} and \textbf{Fig. \ref{mnist:loss}}, respectively. Both plots show a healthy network, reaching an accuracy of close to 100\% shortly after training for three epochs. Even though the training was continued for another three epochs, the network show no signs of overfitting as the validation accuracy keeps being high. 

%For a simple example like the MNIST toy problem, however, ...

\subsubsection{Layer Activations}

In \textbf{Fig. \ref{mnist:layer-act-all}}, we see the layer activations for most layers of the MNIST network. The first convolutional layer does not seem to detect particularly interesting features, but the second convolutional layer detects edges in various locations, which is typical for the first couple of layers of a CNN. The black feature maps indicate that they activate on something that is not found in the selected visualization input image, perhaps edges in the middle of the image. The rest of the layer activations do not provide any specifically insights, but overall they give an impression of the network behavior resulting in a correct classification of the image with 100\% certainty, seen as a single bright activation in the output layer. \\

\noindent \textbf{Fig. \ref{mnist:layer-act-conv2}} shows how the layer activations develop as the training progresses. At stage 1, the convolutional layer has not yet learned the features. We see a significant improvement in stage 5, where the edges in the input image emerge. From stage 5 to stage 10, there are no considerable improvements, as the network has stabilized. \\

\noindent In \textbf{Fig. \ref{vgg:layer-act}}, we see the activations in the second convolutional layer of the VGG16 network. Studying the feature maps and comparing them to the visualization input image, tells us that they do not only detect edges, but also color. For instance, the feature map at position (3,3) seems to activate greatly on the color pink. The feature map at (4,4) activates on light gray/blue, as both the sweater and some parts of the concrete wall is activated. Several of the maps detect the green color of the grass, and so on. 

%The later layers will combine these features to detect more complex features.


%One dangerous pitfall that can be easily noticed with this visualization is that some activation maps may be all zero for many different inputs, which can indicate dead filters, and can be a symptom of high learning rates.
%--> ikke mulig å se flere inputs samtidig

\subsubsection{Saliency Maps}

The evolution of the saliency maps throughout training for the MNIST network is shown in \textbf{Fig. \ref{mnist:saliency}}. We see that at stage 0, which is before any training has been conducted, the saliency appears to be randomly computed, except for the black areas where the gradients are zero. The random computation of the saliency is coherent with the random prediction of the input image as the network has not seen any images yet.\\

\noindent Looking at the following stages, it is evident that the most prominent changes happen during the first stages of training. From stage 7 to stage 10, there are only small adjustments present. The reason for this is that the loss stabilizes after some time. Recall that the saliency is the gradient of the loss with respect to the input image, thus as the loss stabilizes so will the gradient and consequently the saliency. \\

\noindent As seen in \textbf{Fig. \ref{mnist:input-images}}, all visualization input images were classified correctly at stage 1, except for digit 5. Looking at the stage 1 saliency map of digit 5, we see two highlighted spots at the top right and bottom left. These are the parts that the network deems as most important when classifying the input image. In this case, the input image was classified as the digit '2', and we see that the highlighted areas are somewhat consistent with the shape of the number 2, as well as the highlighted areas of the stage 1 saliency map for digit 2. \\

% avslutt dette avsnittet - gir mening at nettverket klassifierte som 2
% bildet for 5 er litt uvanlig. skiller seg kanskje ut blant de 5-tall den er trent på - ikke generell nok enda.

\noindent Another interesting observation is that for many of the input images, the first couple of stages resembles the input image the most. For instance, for digit 3, the saliency map of stage 1 tracks the input image fairly well, while stage 10 presents a significantly more noisy and confusing saliency map. As the training progresses, the network sees more and more images and thus it generalizes to these images. The final saliency maps show us that the network could be looking mostly for certain characteristics instead of the whole picture when classifying the numbers. This is especially evident for digits 1, 4 and 7. For digit 1, we see that its last couple of saliency maps does not highlight the digit from the very bottom to the top. It focuses on the middle part of the line. This could be because the 1s that the network has seen all are of various length and thus the common and most important part influencing the classification is the middle. Similarly, the final saliency maps of digit 4 focus on the two vertical top lines, while the final saliency maps of digit 7 focus on the top horizontal line and practically ignores the diagonal line. \\

\noindent In addition to the saliency maps for the MNIST network, three saliency maps were also presented for the VGG16 network in \textbf{Fig. \ref{vgg:saliency}}. The input images of VGG16 are far more complex than those of MNIST, which is also evident in their saliency maps. It is hard to see any obvious shapes in the saliency maps, but they still indicate what areas of the image were most important. Note that the network was uncertain in its classification of the last two images, which can also explain why the saliency maps are not too clear. However, the top image was classified as a tricycle with 99.95\% certainty. Its saliency map shows us that the focus was definitely on the little girl and especially on how her legs are positioned. This may tell us that the network recognizes ...

% mener at et barn som sitter 

% noe om at nettverket tenker at et barn som sitter på denne måten betyr at det er en tricycle. kan også ta med noe om den streken, at det er ved en gatekant eller noe.

%many images of tricycles also includes children sitting on the tricycle. The slightly skewed vertical line through the saliency map can 

%a: tricycle 99.95
%b: cliff 64.35
%c: sombrero 50.91

\begin{comment}
The saliency map indicates to what degree each pixel in the visualization image influences the specific class outcome. The important regions are easily identified by their brightness. The saliency map can help you to see which parts of the visualization image that your network deems more important when deciding upon the classification score chosen.
\end{comment}

\subsubsection{Deconvolutional Network}

For the MNIST network, only four simple visualizations produced using a deconvolutional network were presented. The visualizations were shown in \textbf{Fig. \ref{mnist:deconv}}, and clearly illustrate that the feature maps are looking for simple edges at various locations in the input image. \\

\noindent \textbf{Fig. \ref{vgg:deconv-block5}} presents visualizations using a deconvolutional network for feature maps in the last pooling layer of the VGG16 network. Naturally, these feature maps seem to be looking for more complex features such as wheels (number 348, 422 and 450) and faces (number 380). In \textbf{Fig. \ref{vgg:deconv-block5-comparison}} we can confirm our interpretation for feature map number 348 and 380. The feature maps surely focus on the wheels and face present in the two images. We can be even more specific in our interpretation, and say that feature map number 348 looks for front wheels of bicycles, which is consistent with both visualizations of the specific feature map. \\

\noindent In \textbf{Fig. \ref{vgg:deconv-block3-comparison}}, we see visualizations using the deconvolution technique for a lower layer. The visualizations shows that the feature maps of this layer looks for more specific, but still complex, features. The selection presents feature maps that are looking for wheels. The comparison shows that the feature maps looks for wheels in similar locations and for similar regions. \\

\noindent Visualizations for the lowest pooling layer of the VGG16 network are seen in \textbf{Fig. \ref{vgg:deconv-block1}}. These feature maps are looking for basic features such as colors and edges. For instance, feature map number 18 obviously looks for the color pink, feature map number 29 looks for a thin line, and number 47 for a rounded corner, and so on.

\begin{comment}
The visualizations show the pattern in the visualization image that was responsible for eliciting activations in the specific feature map. This pattern can reveal what a certain part of the network finds important and what it is looking for in an image. The tools of the figures are linked together similar to the saliency map figures. Additionally, if you click the save tool, all of the deconvolutional visualizations will be downloaded.
\end{comment}

\subsubsection{Deep Visualization}

\textbf{Fig. \ref{mnist:deepvis-output}} shows the evolution of the deep visualizations for the various visualization input images of the MNIST network. The same trend is evident here as in the other techniques described: the biggest changes happen during the first stages. We also see that as the training progresses and the network sees more examples, the deep visualizations actually seem to look less like the actual number. For instance, digits 1 and 5 start looking kind of weird at stage 10. This may be because the network is getting more specialized and learns that the digits can be written with larges variations. The digit 1 might be written straight from top to bottom, or slightly tilted from right to left, which many right-handed commonly do. Another detail worth noting is that the right line of digit 4 is much more noticeable on the upper half than the bottom half. We saw this trend in the saliency maps as well, where the network deemed some regions of the digits more important than others. It is evident that the bottom of the line is not a critical factor for classifying a digit as 4, perhaps because it is present in a number of other digits as well, such as 7 and 9.\\

\noindent Deep visualizations of units in lower layers of the MNIST network are shown in \textbf{Fig. \ref{mnist:deepvis-fc2}} and \textbf{Fig. \ref{mnist:deepvis-fc1}}. We can see patterns in these visualizations that resemble parts of the visualizations in the output layer, but none of them look like complete digits. \\

% something about the black unit??

\noindent The deep visualizations of units in the output layer of the VGG16 network, seen in \textbf{Fig. \ref{vgg:deepvis-output}}, are significantly more complicated. Recall that deep visualizations are synthesized images optimized to maximally activate their corresponding unit. These visualizations show what the various output classes are looking for in an image to be classified as that class. Most of the visualizations are rather straightforward. For instance, the tarantula class visualization contains furry insect legs and prominent spider shapes. The starfish class visualization clearly shows star-shaped objects with a scale-like texture, and the bow tie class visualization contains several different patterns such as stripes, dots and checkers. Some of the visualizations, however, contain parts that we may not immediately see as directly related to its class, but that are common in the context of the class. A good example is the volleyball class visualization. There are several round shapes in the image, but they are not particularly dominant. We can also see what resembles hands, stretching up from the bottom. In addition, the visualization is covered in a texture that appears to be a volleyball net. This shows that it is the combination of several objects that results in a classification. The presence of other objects related to a specific output class improves the certainty that the image belongs to that class. This can also be seen in both the bubble and umbrella class visualizations. Parts of faces are noticeable, because there is usually a person blowing the bubbles, or a person standing beneath the umbrella. Together with the umbrella, we also see the texture of raindrops, and an umbrella is typically covered in rain. \\

\noindent The deep visualization technique incorporate an element of randomness into the creation of visualizations. This is shown in \textbf{Fig. \ref{vgg:deepvis-output-variations}}. The tarantula class visualization is produced twice with the exact same hyperparameter settings. We see that the images are not identical, but they definitely illustrate similar characteristics. In \textbf{Fig. \ref{vgg:deepvis-output-hyperparameters}}, four versions of the tarantula class version is produced, this time with various combinations of hyperparameters. \\ % MIKAL skrive noe mer her??

\noindent \textbf{Fig. \ref{vgg:deepvis-fc2}} and \textbf{Fig. \ref{vgg:deepvis-fc1}} show visualizations of the lower fully connected layers. These are challenging to make sense of, since we do not have a definite answer of what the unit is looking for, as we did in the output layer. Also, we have not seen all 1000 output classes, which makes it difficult to identify similarities to the output class visualizations. By studying the visualizations closely, we can still make some assumptions and speculations of what the units are looking for. The first two visualizations seem to resemble birds of some kind, which is reasonable knowing that there are a number of different bird species among the output classes. The third visualization contains something that resembles leaves or insect wings. Recall that the features of the units in underlying layers are further combined in order to construct the final features of the output layer. The lower units thus contain features for several different output classes. This is especially evident in the chaotic visualizations of units in fc1. In the two images, we can see green snakeskin and some sort of a furry body, perhaps the back of a gorilla. \\ % mangler en liten avslutning her

\noindent \textbf{Fig. \ref{vgg:deepvis-blocks}} illustrate how the deep visualizations change in size and complexity as you go further down into the network. Units in block 2 clearly identifies simpler features than block 5. In \textbf{Fig. \ref{vgg:deepvis-block5-lr-variations}}, we see how changing the learning rate and number of iterations affect the units of a lower layer. % MIKAL skriv noe her? \\
Finally, \textbf{Fig. \ref{vgg:deepvis-block1}} once again confirms that the lower layers looks for the more fundamental features.

\subsection{Visualizations in the Visualization Tool}

By interpreting the different visualizations, we have seen that there are definitely many insights to gain from using the presented visualization techniques. The visualizations provide us with hints on how the ANN is behaving as well as why it is behaving in such a way. We have also seen that examining the visualizations as the training progresses give an understanding of both how the learning 

The training progress mainly indicates how well the network is performing, while the other techniques all focus on uncovering what the various units and feature maps are actually looking for, only from different perspectives. The knowledge learned from studying each of the visualization techniques may overlap, but combining the various approaches allow for seeing the whole picture.


% det er åpenbart at visualiseringene kan gi oss innsikt i hvordan et nettverk fungerer.
% ser også at visualisering underveis i treningsprosessen gir innsikt i utviklingen
% ofte er det nyttig å sammenlikne bilder, som vi ser i resultatene - ikke mulig. (future work)
% kan også være nyttig å kunne fjerne og legge til visualisering for units/layers underveis (future work)
% vanskelig (?) å konstruere et eksempelnettverk som ikke gjør det bra slik at man kan se dette i visualiseringene - vanskelig å peke på spesifikke ting som er dårlig.
% fokuset har vært på å få en forståelse - denne forståelsen kan helt sikkert brukes til å forbedre et nettverk som ikke performer like bra som den kan.
\begin{comment}

RQ 1:How can we develop a visualization tool to improve the understanding of the behaviour of an artificial neural network?

RO 1.1: Examine existing tools in order to uncover shortcomings.
RO 1.2: Identify visualization techniques that can be used to improve the understanding of an artificial neural network.
RO 1.3: Develop a tool that incorporates the techniques found in 1.2 and addresses the shortcomings identified in 1.1.

\end{comment}

% layer act: show what part of an image activates the feature maps

% saliency map: The saliency map indicates to what degree each pixel in the visualization image influences the specific class outcome. The important regions are easily identified by their brightness. The saliency map can help you to see which parts of the visualization image that your network deems more important when deciding upon the classification score chosen.

% deconv: The visualizations show the pattern in the visualization image that was responsible for eliciting activations in the specific feature map. This pattern can reveal what a certain part of the network finds important and what it is looking for in an image.

% deepvis: The synthesized images are optimized to maximally activate their corresponding unit. In other words, they show what the selected units are looking for in an image.


\section{Case Study in Face Recognition}

\begin{comment}
RQ 2: How can facial expression data be utilized to improve a face recognition system?

RO 2.1:Investigate what happens when incorporating expression data in a face recognition system.

order of performance, from best to worst: E, F, B, D, C, A, H, G, I
avvik i orden: test acc. er litt høyere i C enn D
avvik i orden: test og val. loss er litt lavere i A enn C
avvik i orden: test og val. loss er litt lavere i I enn G
avvik i orden: test ID acc. er litt høyere i I enn G
avvik i orden: test og val. exp. acc. er litt høyere i I enn G
avvik i orden: test og val. exp. loss er litt lavere i I enn G
avvik i orden: I har faktisk høyest exp. test acc.

for extra output: loss is high, but accuracy isnt THAT bad

The combined metrics of the extra output architecture make it harder to compare to the baseline and extra input architecture.

The extra output architecture underperforms in comparison with the other two, but still manages an impressive accuracy considering it is tackling two problems simultaneously.

comment on separated performance metrics for extra output networks
\end{comment}

The case study networks included in section \ref{sec:case-results} does not all represent the most promising architectures. We have three architecture types: the baseline, the architecture with extra input, and the architecture with extra output. For each of these, three configurations were included: a minimal, a preferred, and an interesting alternative. The minimal configurations were A, D and G, while the preferred were B, E and H, and lastly, the alternate ones were C, F, and I. Before comparing the three architecture approaches, we first compare the chosen configurations within each approach.

\subsection{Baseline Networks}

The baseline networks were created using configurations A, B and C. The difference between these networks is their depth, where B is deeper than A, and C is deeper than B. As can be seen in \textbf{Table \ref{tab:case-results}}, the accuracy achieved by the minimal network A is relatively high. This, combined with the fact that this network only has a single weight layer, demonstrates the network's ability to easily discern valuable information from the features outputted from the pretrained network. By extension, it confirms the pretrained network's suitability as a feature extractor. To improve the performance, B adds depth, resulting in a more complex classifier. The increased complexity allows the network to interpret the feature input more thoroughly and better tune itself towards images in the IMFDB dataset. The result is a rise in accuracy and a decline in loss compared to A. C, however, which is the deepest baseline network, achieve poorer results than B in general, and has larger loss values than A. A reasonable explanation is that the added parameters enable the network to overfit to the small dataset. The improved accuracy over A is likely caused by outlier examples being correctly classified with low confidence.

\subsection{Extra Input Networks}

Configurations D, E and F were used to create the extra input networks. These differ not only in depth, but also in where the additional input is supplied. E and F have the equally deep, with D being shallower than both. Using a minimal configuration, D has no other option than to inject the extra input right at the start of the network. E and F introduce the expression input at different depths, which marks the main distinction between their architectures. From \textbf{Table \ref{tab:case-results}}, we can see that the complexity gained by adding more layers has a beneficial effect on performance, as both the deeper networks outperform the shallow one, in all aspects. The network from configuration D has inferior performance due to its sole weight layer, the output, which needs to make sense of the combined inputs without any additional processing. E and F does have extra processing, but distinct kinds of processing. In F, the expression input is provided early on in the network, forcing every weighted layer in the model process the information from the feature and expression inputs simultaneously. E, however, injects the expression input later, leaving earlier layers free to exclusively process the feature input. With the performance of E being unequivocally better than F's, the processing of feature input in isolation appear to have advantageous effects. A possibility is that the computations on the feature input alone refines the features, which then eases the subsequent simultaneous processing. 

\subsection{Extra Output Networks}

The extra output network were based on architecture configurations G, H and I. They have varying depth, and different specialized processing parts. H and I are equally deep, both having more layers than G. These two are also similar in the early parts of the networks, with two weighted layers that are used for classifying both identification and expression. However, H utilizes additional specialized processing for the expression output, while I uses the equivalent for the identification output. G has no such components, and rather computes both classification outputs immediately. The identification and expression specific results can be found in \textbf{Table \ref{tab:case-results-id}} and \textbf{Table \ref{tab:case-results-exp}}, respectively. Together, these results reveal how the architectural decisions affect performance. \\

\noindent H stands out as the top performer, with superior values for both loss and accuracy when classifying identification. Looking at the expressions metrics, it also has the lowest expression output loss. Despite this, its accuracy in expression classification is not far off from I's. As mentioned before, this could be caused by the larger loss network, in this case I, correctly classifying outlier examples with low confidence. In H, the feature input is passed through two common weight layers before being presented to the separate output layers. These layers are influenced by both output layers during training, and are subsequently forced to produce information that is valuable to the two classification tasks simultaneously. It is here the use of expression data has the possibility to influence the performance of the network, by potentially revealing a pattern in how the variations in facial features that map to the same identity coincide with the variations in the expected expression outcome. The specialized processing for expression classification in H comes from an additional weight layer which is only affected by the expression output layer when training. Network I has similar shared layers as H, but uses specialized processing for the identification classification instead of for expressions. The common layers in I do improve expression performance compared to G, but, peculiarly, these layers combined with the specialized processing do not contribute to an definite advantage in identification classification. Again, I shows somewhat similar accuracy, but has a larger loss. As mentioned in section \ref{case-networks}, the lack of an identification performance boost is likely caused by overfitting due to the specialized processing. 

\subsection{Comparing Approaches}

To compare the extra output approach to the others, we focus on the identification performance. When looking at the architectures of the networks, D and G resemble the baseline configuration A. Of these, D is the most successful performance-wise, with A and G showing similar performances. The latter comes as no surprise, considering that the input in G is not processed by any weight layers are shared between the identification and expression output layers. Consequently, there are no common layers which both affect in the learning procedure, and the classification of identification and expression is executed strictly separately. Classifying the identification is then performed similarly in A and G. The sole difference between A and D is the extra input D receives. D's utilization of this additional information is the only architectural explanation for its increase in performance. \\

\noindent The situation is similar to how E and F compare to B. The enhanced performance of E and F over D solidifies the interpretation of B's improvement over A, that the added layers allow for increased complexity and potential tuning to our particular input. However, in the extra input architectures, the more complex systems have access to more information, and, as implied by the advance in performance, they find a meaningful use for it. With the addition of the expression information being the only architectural difference from B, it must be the cause of the performance gain. All of the baseline configurations can then be improved upon by including an additional expression input. \\

\noindent The last baseline configuration, C, is comparable to H and I. Among these, H has the most promising results, with C performing slightly above I, only notably surpassing it when regarding the loss calculated from the validation set. While H is as deep as C, its full depth is only utilized to classify the expression, and the identification result is computed after the first two weight layers. Since we are considering the extra output architectures based their identification accuracy, H could be compared to B, by disregarding the expression specific processing part. These two have quite similar results, with H having slightly higher accuracy, but also slightly larger loss values. Because of the meager differences, we cannot decisively say that one configuration is preferred over the other based on their performance. However, if we include complexity, B is the better choice. Not only does its architecture contain less parameters, it also requires less data in training. \\

\noindent Of the three approaches, the extra input architectures have the most impressive results. Every configuration in this approach outperforms their corresponding baseline alternatives. Compared to the extra output architectures, they are also generally superior, with the exception of the minimal configuration D versus H. However, the most interesting architectures consist of the preferred configuration of each approach, namely B, E and H. E is definitively the best choice among these, with overall better performance measurements, proving that the extra input architecture is the approach to prefer. In addition, B is favored over H, as previously discussed, making the extra output architecture the least promising alternative, even after the vanilla baseline approach. These results demonstrate that a face recognition system can be improved by exploiting facial expression data, and that an advantageous approach is to include the expression information as an additional network input.


\cleardoublepage